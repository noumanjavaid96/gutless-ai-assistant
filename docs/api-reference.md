# API Reference

This document provides detailed API documentation for the Renesis AI Assistant's internal modules and external integrations.

## Core Modules

### Bot Interface (`src/bot.py`)

#### SlackBot Class

```python
class SlackBot:
    """Main Slack bot interface for handling user interactions."""
    
    def __init__(self, token: str, signing_secret: str, app_token: str)
    def start(self) -> None
    def handle_message(self, message: dict) -> None
    def handle_app_mention(self, event: dict) -> None
    def send_response(self, channel: str, text: str, thread_ts: str = None) -> None
```

**Methods:**

##### `__init__(token, signing_secret, app_token)`
Initializes the Slack bot with required credentials.

**Parameters:**
- `token` (str): Slack bot token (xoxb-...)
- `signing_secret` (str): Slack signing secret for request verification
- `app_token` (str): Slack app token for Socket Mode (xapp-...)

##### `start()`
Starts the bot and begins listening for Slack events.

**Returns:** None

##### `handle_message(message)`
Processes incoming direct messages.

**Parameters:**
- `message` (dict): Slack message event data

**Returns:** None

##### `handle_app_mention(event)`
Processes messages where the bot is mentioned.

**Parameters:**
- `event` (dict): Slack app mention event data

**Returns:** None

##### `send_response(channel, text, thread_ts=None)`
Sends a formatted response to Slack.

**Parameters:**
- `channel` (str): Slack channel ID
- `text` (str): Response text (supports Slack markdown)
- `thread_ts` (str, optional): Thread timestamp for threaded replies

**Returns:** None

### Core Logic (`src/core_logic.py`)

#### QueryProcessor Class

```python
class QueryProcessor:
    """Orchestrates the query-to-answer pipeline."""
    
    def __init__(self, embedding_service: EmbeddingService, 
                 pinecone_service: PineconeService)
    def process_query(self, query: str, user_id: str = None) -> QueryResponse
    def preprocess_query(self, query: str) -> str
    def assemble_context(self, search_results: List[SearchResult]) -> str
    def generate_answer(self, query: str, context: str) -> str
    def format_response(self, answer: str, sources: List[Source]) -> QueryResponse
```

**Methods:**

##### `process_query(query, user_id=None)`
Main method that processes a user query and returns a complete response.

**Parameters:**
- `query` (str): User's question
- `user_id` (str, optional): Slack user ID for logging

**Returns:** `QueryResponse` object

**Example:**
```python
processor = QueryProcessor(embedding_service, pinecone_service)
response = processor.process_query("What foods should I avoid in Week 1?")
print(response.answer)
print(response.sources)
```

##### `preprocess_query(query)`
Cleans and normalizes the user query.

**Parameters:**
- `query` (str): Raw user query

**Returns:** str - Cleaned query

##### `assemble_context(search_results)`
Combines relevant search results into context for the LLM.

**Parameters:**
- `search_results` (List[SearchResult]): Results from vector search

**Returns:** str - Assembled context

##### `generate_answer(query, context)`
Generates an answer using the LLM with the provided context.

**Parameters:**
- `query` (str): User's question
- `context` (str): Relevant context from search results

**Returns:** str - Generated answer

##### `format_response(answer, sources)`
Formats the final response with sources and metadata.

**Parameters:**
- `answer` (str): Generated answer
- `sources` (List[Source]): Source references

**Returns:** `QueryResponse` object

### Embedding Service (`src/embedding_service.py`)

#### EmbeddingService Class

```python
class EmbeddingService:
    """Handles text embedding generation using OpenAI."""
    
    def __init__(self, api_key: str, model: str = "text-embedding-ada-002")
    def generate_embedding(self, text: str) -> List[float]
    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]
    def get_embedding_dimension(self) -> int
```

**Methods:**

##### `generate_embedding(text)`
Generates a single embedding for the provided text.

**Parameters:**
- `text` (str): Text to embed

**Returns:** List[float] - 1536-dimensional embedding vector

**Example:**
```python
embedding_service = EmbeddingService(api_key)
vector = embedding_service.generate_embedding("What is gut health?")
print(len(vector))  # 1536
```

##### `generate_embeddings_batch(texts)`
Generates embeddings for multiple texts efficiently.

**Parameters:**
- `texts` (List[str]): List of texts to embed

**Returns:** List[List[float]] - List of embedding vectors

##### `get_embedding_dimension()`
Returns the dimension of embeddings generated by the current model.

**Returns:** int - Embedding dimension (1536 for text-embedding-ada-002)

### Pinecone Service (`src/pinecone_service.py`)

#### PineconeService Class

```python
class PineconeService:
    """Manages vector database operations with Pinecone."""
    
    def __init__(self, api_key: str, environment: str, index_name: str)
    def upsert_vectors(self, vectors: List[Vector]) -> None
    def search(self, query_vector: List[float], top_k: int = 5, 
               filter_dict: dict = None) -> List[SearchResult]
    def delete_vectors(self, ids: List[str]) -> None
    def get_index_stats(self) -> dict
```

**Methods:**

##### `upsert_vectors(vectors)`
Inserts or updates vectors in the index.

**Parameters:**
- `vectors` (List[Vector]): List of Vector objects to upsert

**Returns:** None

**Example:**
```python
vectors = [
    Vector(
        id="chunk_1",
        values=embedding_vector,
        metadata={
            "source": "week1_video.mp4",
            "text": "Content chunk text",
            "timestamp_start": "00:05:30"
        }
    )
]
pinecone_service.upsert_vectors(vectors)
```

##### `search(query_vector, top_k=5, filter_dict=None)`
Searches for similar vectors in the index.

**Parameters:**
- `query_vector` (List[float]): Query embedding vector
- `top_k` (int): Number of results to return
- `filter_dict` (dict, optional): Metadata filters

**Returns:** List[SearchResult] - Ranked search results

**Example:**
```python
results = pinecone_service.search(
    query_vector=query_embedding,
    top_k=5,
    filter_dict={"content_type": "transcript"}
)
```

##### `delete_vectors(ids)`
Deletes vectors from the index.

**Parameters:**
- `ids` (List[str]): List of vector IDs to delete

**Returns:** None

##### `get_index_stats()`
Returns statistics about the index.

**Returns:** dict - Index statistics

### Content Ingestion (`src/content_ingestion/`)

#### TranscriptionService Class

```python
class TranscriptionService:
    """Handles audio/video transcription."""
    
    def __init__(self, service_type: str = "whisper")
    def transcribe_file(self, file_path: str) -> TranscriptionResult
    def transcribe_url(self, url: str) -> TranscriptionResult
    def segment_transcript(self, transcript: str, 
                          segment_length: int = 300) -> List[TranscriptSegment]
```

#### PDFParser Class

```python
class PDFParser:
    """Handles PDF content extraction."""
    
    def __init__(self)
    def extract_text(self, file_path: str) -> PDFContent
    def extract_with_metadata(self, file_path: str) -> PDFContent
    def chunk_content(self, content: str, chunk_size: int = 1000) -> List[ContentChunk]
```

#### ContentProcessor Class

```python
class ContentProcessor:
    """Main content ingestion orchestrator."""
    
    def __init__(self, transcription_service: TranscriptionService,
                 pdf_parser: PDFParser, embedding_service: EmbeddingService,
                 pinecone_service: PineconeService)
    def process_video(self, file_path: str, metadata: dict) -> ProcessingResult
    def process_pdf(self, file_path: str, metadata: dict) -> ProcessingResult
    def process_batch(self, file_list: List[FileInfo]) -> List[ProcessingResult]
```

## Data Models

### QueryResponse

```python
@dataclass
class QueryResponse:
    """Response object for processed queries."""
    answer: str
    sources: List[Source]
    confidence: float
    processing_time: float
    metadata: dict
```

### SearchResult

```python
@dataclass
class SearchResult:
    """Result from vector similarity search."""
    id: str
    score: float
    text: str
    metadata: dict
```

### Source

```python
@dataclass
class Source:
    """Source reference for answers."""
    type: str  # "video" or "pdf"
    name: str
    url: str
    timestamp: str = None  # For videos
    page: int = None  # For PDFs
    section: str = None
```

### Vector

```python
@dataclass
class Vector:
    """Vector object for Pinecone operations."""
    id: str
    values: List[float]
    metadata: dict
```

### TranscriptionResult

```python
@dataclass
class TranscriptionResult:
    """Result from transcription service."""
    text: str
    segments: List[TranscriptSegment]
    language: str
    confidence: float
```

### TranscriptSegment

```python
@dataclass
class TranscriptSegment:
    """Individual segment of transcribed content."""
    text: str
    start_time: float
    end_time: float
    speaker: str = None
```

### PDFContent

```python
@dataclass
class PDFContent:
    """Extracted PDF content."""
    text: str
    pages: List[PageContent]
    metadata: dict
```

### ContentChunk

```python
@dataclass
class ContentChunk:
    """Processed content chunk."""
    text: str
    metadata: dict
    embedding: List[float] = None
```

## External API Integrations

### OpenAI API

#### Embeddings Endpoint
```python
# Generate embeddings
response = openai.Embedding.create(
    model="text-embedding-ada-002",
    input=["text to embed"]
)
embedding = response['data'][0]['embedding']
```

#### Chat Completions Endpoint
```python
# Generate chat completion
response = openai.ChatCompletion.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "User question with context"}
    ],
    max_tokens=1000,
    temperature=0.7
)
answer = response.choices[0].message.content
```

### Pinecone API

#### Index Operations
```python
# Initialize index
index = pinecone.Index("gutless-assistant")

# Upsert vectors
index.upsert(vectors=[
    {
        "id": "chunk_1",
        "values": embedding_vector,
        "metadata": {"source": "video.mp4"}
    }
])

# Query index
results = index.query(
    vector=query_vector,
    top_k=5,
    include_metadata=True
)
```

### Slack API

#### Events API
```python
# Handle app mention event
@app.event("app_mention")
def handle_mention(event, say):
    user_query = extract_query(event["text"])
    response = process_query(user_query)
    say(response)
```

#### Web API
```python
# Send message
client.chat_postMessage(
    channel=channel_id,
    text=response_text,
    thread_ts=thread_timestamp
)
```

## Error Handling

### Custom Exceptions

```python
class GutlessAssistantError(Exception):
    """Base exception for the application."""
    pass

class EmbeddingError(GutlessAssistantError):
    """Error in embedding generation."""
    pass

class SearchError(GutlessAssistantError):
    """Error in vector search."""
    pass

class TranscriptionError(GutlessAssistantError):
    """Error in transcription process."""
    pass
```

### Error Response Format

```python
@dataclass
class ErrorResponse:
    """Standardized error response."""
    error_type: str
    message: str
    details: dict
    timestamp: datetime
```

## Rate Limiting

### OpenAI Rate Limits
- Embeddings: 3,000 requests/minute
- Chat Completions: 3,500 requests/minute
- Tokens per minute: 90,000

### Pinecone Rate Limits
- Queries: 100 requests/second
- Upserts: 100 requests/second

### Slack Rate Limits
- Web API: 1+ requests/second (tier-based)
- Events API: No specific limit

## Authentication

### API Key Management
```python
# Environment variables
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
SLACK_BOT_TOKEN = os.getenv("SLACK_BOT_TOKEN")
```

### Request Authentication
```python
# OpenAI
openai.api_key = OPENAI_API_KEY

# Pinecone
pinecone.init(api_key=PINECONE_API_KEY, environment=ENVIRONMENT)

# Slack
app = App(token=SLACK_BOT_TOKEN)
```

## Next Steps


- [Troubleshooting](troubleshooting.md)